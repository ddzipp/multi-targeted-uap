{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Qwen2.5-vl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30e9cc595da144209931fd501b103069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "import torch\n",
    "# default: Load the model on the available device(s)\n",
    "# model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "#     \"Qwen/Qwen2.5-VL-3B-Instruct\", torch_dtype=\"auto\", device_map=\"cpu\"\n",
    "# )\n",
    "\n",
    "# We recommend enabling flash_attention_2 for better acceleration and memory saving, especially in multi-image and video scenarios.\n",
    "model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "    \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# default processer\n",
    "processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_dataset\n",
    "dataset = load_dataset(\"ImageNet\", split='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.6000, 0.3451, 0.5059,  ..., 0.1216, 0.1255, 0.1176],\n",
       "         [0.3137, 0.3020, 0.3216,  ..., 0.1255, 0.1333, 0.1255],\n",
       "         [0.4706, 0.2000, 0.2941,  ..., 0.1255, 0.1412, 0.1373],\n",
       "         ...,\n",
       "         [0.2745, 0.2627, 0.2706,  ..., 0.0627, 0.0510, 0.0275],\n",
       "         [0.2275, 0.2157, 0.2235,  ..., 0.0667, 0.0588, 0.0431],\n",
       "         [0.1686, 0.1765, 0.1647,  ..., 0.0824, 0.0667, 0.0588]],\n",
       "\n",
       "        [[0.7843, 0.4627, 0.6039,  ..., 0.1098, 0.1216, 0.1255],\n",
       "         [0.4941, 0.3882, 0.3725,  ..., 0.1176, 0.1294, 0.1255],\n",
       "         [0.6314, 0.2745, 0.3098,  ..., 0.1216, 0.1333, 0.1294],\n",
       "         ...,\n",
       "         [0.2745, 0.2667, 0.2745,  ..., 0.0588, 0.0510, 0.0275],\n",
       "         [0.2275, 0.2235, 0.2314,  ..., 0.0627, 0.0588, 0.0431],\n",
       "         [0.1725, 0.1843, 0.1725,  ..., 0.0824, 0.0627, 0.0549]],\n",
       "\n",
       "        [[0.4392, 0.2706, 0.4157,  ..., 0.1059, 0.0941, 0.0863],\n",
       "         [0.2235, 0.2353, 0.2902,  ..., 0.1098, 0.1098, 0.0980],\n",
       "         [0.3843, 0.1647, 0.2196,  ..., 0.1059, 0.1137, 0.0980],\n",
       "         ...,\n",
       "         [0.2392, 0.2314, 0.2353,  ..., 0.0549, 0.0471, 0.0196],\n",
       "         [0.2000, 0.1922, 0.2039,  ..., 0.0588, 0.0510, 0.0353],\n",
       "         [0.1490, 0.1608, 0.1490,  ..., 0.0745, 0.0549, 0.0471]]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = dataset[1]['image']\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "text = processor.apply_chat_template(messages)\n",
    "inputs = processor(text=text, images=image, return_tensors=\"pt\", do_rescale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model.generate(**inputs, max_new_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['system\\nYou are a helpful assistant.\\nuser\\nDescribe this image.\\n fishing']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([1, 143])\n",
      "torch.Size([484, 1176])\n",
      "tensor([[ 1, 22, 22]])\n"
     ]
    }
   ],
   "source": [
    "print(inputs.keys())\n",
    "print(inputs['input_ids'].shape)\n",
    "print(inputs['attention_mask'].shape)\n",
    "print(inputs['pixel_values'].shape)\n",
    "print(inputs['image_grid_thw'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "<|vision_start|><|image_pad|><|vision_end|>Describe this image.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The default range for the number of visual tokens per image in the model is 4-16384.\n",
    "# You can set min_pixels and max_pixels according to your needs, such as a token range of 256-1280, to balance performance and cost.\n",
    "# min_pixels = 256*28*28\n",
    "# max_pixels = 1280*28*28\n",
    "# processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"type\": \"image\",\n",
    "                \n",
    "            },\n",
    "            {\"type\": \"text\", \"text\": \"Describe this image.\"},\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "# Preparation for inference\n",
    "text = processor.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1365)\n",
      "torch.Size([3, 1365, 2048])\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen-VL/assets/demo.jpeg\"\n",
    "response = requests.get(url)\n",
    "image_raw = Image.open(BytesIO(response.content))\n",
    "print(image_raw.size)\n",
    "import torchvision\n",
    "toTensor = torchvision.transforms.ToTensor() # PIL Image to Tensor\n",
    "image = toTensor(image_raw)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1365, 2048])\n"
     ]
    }
   ],
   "source": [
    "image = image.unsqueeze(0).repeat(2, 1, 1, 1)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([14308, 1176])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "inputs_gt = processor(text=text, images=image_raw, return_tensors=\"pt\")\n",
    "print(inputs_gt['pixel_values'].shape)\n",
    "print(inputs_gt['pixel_values'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28616, 1176])\n",
      "torch.float32\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "inputs = processor.image_processor.preprocess(images=image, return_tensors=\"pt\", do_rescale=False)\n",
    "print(inputs['pixel_values'].shape)\n",
    "print(inputs['pixel_values'].dtype)\n",
    "print((inputs['pixel_values'][:14308,:] == inputs_gt['pixel_values']).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8386560\n",
      "16826208\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8413104"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(3*1365*2048)\n",
    "print(14308* 1176)\n",
    "16826208 // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m a \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m6\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m      3\u001b[0m b \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m \u001b[43ma\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mb\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (6) must match the size of tensor b (2) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.ones(6,2,2)\n",
    "b = torch.ones(2,2,2)\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constraint = Constraint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8501,  0.8501,  0.8647,  ...,  1.3922,  1.3922,  1.3922],\n",
       "        [ 0.9376,  0.9376,  0.9376,  ...,  1.4491,  1.4491,  1.4491],\n",
       "        [ 0.9084,  0.9376,  0.9376,  ...,  1.4065,  1.4207,  1.4207],\n",
       "        ...,\n",
       "        [-0.1280, -0.1280, -0.1426,  ..., -0.2431, -0.2715, -0.3000],\n",
       "        [-0.3324, -0.3324, -0.3032,  ..., -0.3000, -0.2715, -0.2857],\n",
       "        [-0.3762, -0.4054, -0.4054,  ..., -0.4279, -0.4422, -0.4564]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['pixel_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from qwen_vl_utils import process_vision_info\n",
    "image_inputs, video_inputs = process_vision_info(messages)\n",
    "inputs = processor(\n",
    "    text=[text],\n",
    "    images=image_inputs,\n",
    "    videos=video_inputs,\n",
    "    padding=True,\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "inputs = inputs.to(\"cuda\")\n",
    "\n",
    "# Inference: Generation of the output\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "generated_ids_trimmed = [\n",
    "    out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "]\n",
    "output_text = processor.batch_decode(\n",
    "    generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    ")\n",
    "print(output_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
