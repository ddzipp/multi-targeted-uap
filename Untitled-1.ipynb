{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import Normalize\n",
    "\n",
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "## Clip perturbation\n",
    "# mean = torch.tensor([0.48145466, 0.4578275, 0.40821073])\n",
    "# std = torch.tensor([0.26862954, 0.26130258, 0.27577711])\n",
    "\n",
    "inv_mean = torch.tensor([-m / s for m, s in zip(mean, std)])\n",
    "inv_std = torch.tensor([1 / s for s in std])\n",
    "\n",
    "# The bound for the pixel values is [0, 1] in the normalized space\n",
    "min_values = ((torch.zeros(3) - mean) / std).view(3, 1, 1)\n",
    "max_values = ((torch.ones(3) - mean) / std).view(3, 1, 1)\n",
    "\n",
    "\n",
    "def clip_image(normalized_image, normalized=False):\n",
    "    if not normalized:\n",
    "        return torch.clamp(normalized_image, 0, 1)\n",
    "    return torch.clamp(normalized_image, min_values, max_values)\n",
    "\n",
    "\n",
    "norm_fn = Normalize(mean, std)\n",
    "inv_norm_fn = Normalize(inv_mean, inv_std)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.2009,  1.9181, -1.1264,  ...,  0.8074,  1.9287, -1.1417],\n",
       "          [ 0.0608, -1.7421,  1.3180,  ..., -1.9318, -1.0037, -1.2113],\n",
       "          [-0.1728, -0.1751, -0.6632,  ..., -1.5651, -1.3859,  0.6851],\n",
       "          ...,\n",
       "          [ 0.9333, -0.8261, -0.6986,  ..., -1.7029,  1.6132, -0.7223],\n",
       "          [ 1.4096, -0.1904, -0.7313,  ..., -0.7965,  0.8963, -1.3637],\n",
       "          [ 1.3201, -1.5916, -1.8827,  ...,  0.4737,  1.1267, -0.2330]],\n",
       "\n",
       "         [[-1.6995, -1.1657,  1.9551,  ..., -1.9085, -1.3308, -0.0825],\n",
       "          [-1.6274, -0.0263,  0.6062,  ...,  0.9819,  1.1926,  0.6597],\n",
       "          [-1.4457,  1.6926,  0.8611,  ...,  0.8218,  0.6208,  0.0030],\n",
       "          ...,\n",
       "          [ 0.6135, -0.4253,  0.5318,  ..., -0.1441, -1.2338, -0.5111],\n",
       "          [ 1.1184, -1.3830,  1.2614,  ...,  1.1907, -1.9004,  0.4565],\n",
       "          [ 0.2511, -1.3182,  1.0791,  ...,  0.3485, -1.9087,  0.0496]],\n",
       "\n",
       "         [[ 0.5088,  0.9605,  1.4905,  ..., -0.8266, -0.0350,  1.8172],\n",
       "          [ 1.1754, -0.7252, -0.1368,  ..., -1.8145,  1.5841,  1.4632],\n",
       "          [ 0.7957,  1.9809,  1.6851,  ..., -1.9273,  0.8959, -1.7803],\n",
       "          ...,\n",
       "          [ 0.7667, -1.4025, -0.7043,  ...,  0.1766,  0.4044,  0.3010],\n",
       "          [ 0.6377, -0.5663,  0.2330,  ...,  0.0297,  1.3841,  1.7073],\n",
       "          [-1.6060, -1.0799,  1.8347,  ..., -0.6891, -0.8152,  0.7035]]]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rand = torch.rand(1, 3, 224, 224)*4 -2\n",
    "rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "std = torch.tensor([0.229, 0.224, 0.225])\n",
    "clip_mean=  torch.tensor([0.48145466, 0.4578275, 0.40821073])\n",
    "clip_std= torch.tensor([0.26862954, 0.26130258, 0.27577711])\n",
    "norm_fn = Normalize(mean, std)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inv_mean = torch.tensor([-m / s for m, s in zip(mean, std)])\n",
    "inv_std = torch.tensor([1 / s for s in std])\n",
    "inv_norm_fn  = Normalize(inv_mean, inv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2448, 0.2029, 0.3741,  ..., 0.9591, 0.9754, 0.1390],\n",
       "         [0.8355, 0.6505, 0.5030,  ..., 0.1779, 0.3145, 0.8347],\n",
       "         [0.0586, 0.9223, 0.5254,  ..., 0.4686, 0.9266, 0.2433],\n",
       "         ...,\n",
       "         [0.5205, 0.2141, 0.0146,  ..., 0.3290, 0.6824, 0.0391],\n",
       "         [0.9702, 0.5732, 0.0606,  ..., 0.2642, 0.9295, 0.9459],\n",
       "         [0.3801, 0.2709, 0.1265,  ..., 0.1585, 0.7795, 0.4553]],\n",
       "\n",
       "        [[0.5870, 0.3386, 0.0414,  ..., 0.7720, 0.5774, 0.0064],\n",
       "         [0.5576, 0.0774, 0.6841,  ..., 0.6370, 0.3350, 0.1253],\n",
       "         [0.2697, 0.8528, 0.3887,  ..., 0.3016, 0.1661, 0.7333],\n",
       "         ...,\n",
       "         [0.4063, 0.0221, 0.1705,  ..., 0.2064, 0.7245, 0.0334],\n",
       "         [0.3802, 0.2557, 0.9318,  ..., 0.8483, 0.2646, 0.4984],\n",
       "         [0.9216, 0.2097, 0.2276,  ..., 0.6022, 0.7462, 0.7435]],\n",
       "\n",
       "        [[0.9290, 0.9201, 0.5948,  ..., 0.1938, 0.1778, 0.2148],\n",
       "         [0.5918, 0.4112, 0.3273,  ..., 0.0925, 0.8589, 0.0833],\n",
       "         [0.9291, 0.7493, 0.0840,  ..., 0.2967, 0.2658, 0.7303],\n",
       "         ...,\n",
       "         [0.6452, 0.8450, 0.9676,  ..., 0.7290, 0.5970, 0.1116],\n",
       "         [0.4805, 0.6543, 0.3556,  ..., 0.1904, 0.1861, 0.8857],\n",
       "         [0.1665, 0.1478, 0.5737,  ..., 0.8726, 0.3934, 0.2222]]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalized Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.1179, -2.0357, -1.8044])\n",
      "tensor([2.2489, 2.4286, 2.6400])\n"
     ]
    }
   ],
   "source": [
    "print((torch.zeros(3) - mean) / std)\n",
    "print((torch.ones(3) - mean) / std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip Normalization Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.7923, -1.7521, -1.4802])\n",
      "tensor([1.9303, 2.0749, 2.1459])\n"
     ]
    }
   ],
   "source": [
    "print((torch.zeros(3) - clip_mean) / clip_std)\n",
    "print((torch.ones(3) - clip_mean) / clip_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4850, 0.4560, 0.4060])\n",
      "tensor([0.7140, 0.6800, 0.6310])\n"
     ]
    }
   ],
   "source": [
    "print((torch.zeros(3) - inv_mean) / inv_std)\n",
    "print((torch.ones(3) - inv_mean) / inv_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Clip Bound "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4815, 0.4578, 0.4082])\n",
      "tensor([0.7501, 0.7191, 0.6840])\n"
     ]
    }
   ],
   "source": [
    "print((torch.zeros(3) - inv_mean) / inv_std)\n",
    "print((torch.ones(3) - inv_mean) / inv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
