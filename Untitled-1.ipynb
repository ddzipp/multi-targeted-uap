{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 299, 299])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from config import Config\n",
    "from dataset import collate_fn, load_dataset\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    name: str,\n",
    "    sample_id: torch.Tensor,\n",
    "    targets: dict,\n",
    "    split=\"val\",\n",
    "    transform=None,\n",
    "    shuffle=True,\n",
    "    batch_size=5,\n",
    "):\n",
    "    # Set multi-target labels\n",
    "    dataset = load_dataset(name, split=split, targets=targets, transform=transform)\n",
    "    dataset = Subset(dataset, sample_id.flatten().tolist())\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "dataloader = get_dataloader(\n",
    "    cfg.dataset_name,\n",
    "    cfg.sample_id,\n",
    "    cfg.targets,\n",
    "    split=cfg.split,\n",
    "    batch_size=cfg.batch_size,\n",
    ")\n",
    "\n",
    "# Example use\n",
    "test_image = dataloader.dataset[0][\"image\"].unsqueeze(0)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/research/sources/multi-targeted-uap/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Blip2ForConditionalGeneration,\n",
    "    Blip2Processor,\n",
    "    InstructBlipForConditionalGeneration,\n",
    "    InstructBlipProcessor,\n",
    "    LlavaNextProcessor,\n",
    ")\n",
    "\n",
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "llava = AutoProcessor.from_pretrained(model_id)\n",
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "llavanext = LlavaNextProcessor.from_pretrained(model_id)\n",
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "qwen = AutoProcessor.from_pretrained(model_id)\n",
    "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "qwen25 = AutoProcessor.from_pretrained(model_id)\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "llama = AutoProcessor.from_pretrained(model_id)\n",
    "model_id = \"Salesforce/blip2-opt-2.7b\"\n",
    "blip = Blip2Processor.from_pretrained(model_id)\n",
    "model_id = \"Salesforce/instructblip-vicuna-7b\"\n",
    "instructblip = InstructBlipProcessor.from_pretrained(model_id)\n",
    "model_id = \"openvla/openvla-7b\"\n",
    "openvla = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "processors = [llava, llavanext, qwen, qwen25, llama, blip, instructblip, openvla]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[INST] <image>\\nDescribe the iamge [/INST]',\n",
       " '[INST] <image>\\nDescribe the iamge [/INST]',\n",
       " '[INST] <image>\\nDescribe the iamge [/INST]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = [\"Describe the iamge\"] * 3\n",
    "targets = [\"ERROR!\", \"WARNING!\", \"MY TARGET!\"]\n",
    "prompts = []\n",
    "for q in questions:\n",
    "    conv = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": q},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    conversation = llava.apply_chat_template(conv, add_generation_prompt=False)\n",
    "    prompts.append(conversation)\n",
    "\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = llava(\n",
    "    images=test_image,\n",
    "    text=prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    do_rescale=False,  # the image is already rescaled to [0, 1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_inputs(processor, add_generation_prompt=False):\n",
    "    print(\"****************\" * 5)\n",
    "    eos_token = processor.tokenizer.eos_token\n",
    "    print(\"eos_token:\", eos_token)\n",
    "    conv = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Can you tell me what this image is about?\"},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"My Answer.\"}],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conv, add_generation_prompt=add_generation_prompt)\n",
    "    print(prompt)\n",
    "    print()\n",
    "    inputs = processor(text=prompt, images=test_image, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "    print(inputs.keys())\n",
    "    print(inputs[\"input_ids\"].shape)\n",
    "    print((inputs[\"attention_mask\"] == 1).all())\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: </s>\n",
      "[INST] <image>\n",
      "Can you tell me what this image is about? [/INST] My Answer.</s> \n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_sizes'])\n",
      "torch.Size([1, 1202])\n",
      "tensor(True)\n",
      "tensor([[28804,   733, 28748, 16289, 28793,  1984, 26307, 28723,     2,   259]])\n",
      "********************************************************************************\n",
      "eos_token: </s>\n",
      "[INST] <image>\n",
      "Can you tell me what this image is about? [/INST] My Answer.</s> \n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_sizes'])\n",
      "torch.Size([1, 1202])\n",
      "tensor(True)\n",
      "tensor([[28804,   733, 28748, 16289, 28793,  1984, 26307, 28723,     2,   259]])\n"
     ]
    }
   ],
   "source": [
    "inputs = print_inputs(llava)\n",
    "print(inputs[\"input_ids\"][:, -10:])\n",
    "inputs = print_inputs(llava, True)\n",
    "print(inputs[\"input_ids\"][:, -10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: </s>\n",
      "[INST] <image>\n",
      "Can you tell me what this image is about? [/INST] My Answer.</s> \n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_sizes'])\n",
      "torch.Size([1, 1202])\n",
      "tensor(True)\n",
      "********************************************************************************\n",
      "eos_token: </s>\n",
      "[INST] <image>\n",
      "Can you tell me what this image is about? [/INST] My Answer.</s> \n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_sizes'])\n",
      "torch.Size([1, 1202])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "inputs = print_inputs(llavanext)\n",
    "\n",
    "inputs = print_inputs(llavanext, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: <|im_end|>\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me what this image is about?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "My Answer.<|im_end|>\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
      "torch.Size([1, 157])\n",
      "tensor(True)\n",
      "torch.Size([1, 3])\n",
      "********************************************************************************\n",
      "eos_token: <|im_end|>\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me what this image is about?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "My Answer.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
      "torch.Size([1, 160])\n",
      "tensor(True)\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = print_inputs(qwen)\n",
    "print(inputs[\"image_grid_thw\"].shape)\n",
    "inputs = print_inputs(qwen, True)\n",
    "print(inputs[\"image_grid_thw\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: <|im_end|>\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me what this image is about?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "My Answer.<|im_end|>\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
      "torch.Size([1, 157])\n",
      "tensor(True)\n",
      "********************************************************************************\n",
      "eos_token: <|im_end|>\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me what this image is about?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "My Answer.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
      "torch.Size([1, 160])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print_inputs(qwen25)\n",
    "print_inputs(qwen25, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: <|eot_id|>\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me what this image is about?<|image|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My Answer.<|eot_id|>\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask'])\n",
      "torch.Size([1, 26])\n",
      "tensor(True)\n",
      "torch.Size([1, 26, 1, 4])\n",
      "********************************************************************************\n",
      "eos_token: <|eot_id|>\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me what this image is about?<|image|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My Answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask'])\n",
      "torch.Size([1, 30])\n",
      "tensor(True)\n",
      "torch.Size([1, 30, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = print_inputs(llama)\n",
    "print(inputs[\"cross_attention_mask\"].shape)\n",
    "inputs = print_inputs(llama, True)\n",
    "print(inputs[\"cross_attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(openvla.chat_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Target Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me what this?<|image|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask'])\n",
      "tensor([[   757,   1148,    420,     30, 128256, 128009, 128006,  78191, 128007,\n",
      "            271]])\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me what this?<|image|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Target<|eot_id|>\n",
      "tensor([[   420,     30, 128256, 128009, 128006,  78191, 128007,    271,   6531,\n",
      "         128009]])\n",
      "***************\n",
      "Error input_ids\n",
      "Error attention_mask\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "Error cross_attention_mask\n"
     ]
    }
   ],
   "source": [
    "processor = llama\n",
    "conv = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Can you tell me what this?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conv, add_generation_prompt=True)\n",
    "print(prompt)\n",
    "inputs0 = processor(images=test_image, text=prompt, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "print(inputs0.keys())\n",
    "print(inputs0[\"input_ids\"][:, -10:])\n",
    "\n",
    "\n",
    "conv.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Target\"},\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "prompt = processor.apply_chat_template(conv, add_generation_prompt=False)\n",
    "print(prompt)\n",
    "inputs1 = processor(images=test_image, text=prompt, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "print(inputs1[\"input_ids\"][:, -10:])\n",
    "\n",
    "print(\"***************\")\n",
    "for key in inputs1.keys():\n",
    "    try:\n",
    "        print((inputs0[key] == inputs1[key]).all())\n",
    "    except Exception:\n",
    "        print(\"Error\", key)\n",
    "# target = \"Target\" + processor.tokenizer.eos_token\n",
    "# inputs = processor.tokenizer.encode(target, add_special_tokens=False)\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs0[\"cross_attention_mask\"] == inputs1[\"cross_attention_mask\"][:, :19]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 0]],\n",
       "\n",
       "         [[1, 0, 0, 0]]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs1[\"cross_attention_mask\"][:, 19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openvla.tokenizer.eos_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_inputs(processor, add_generation_prompt=True, target=\"Target\"):\n",
    "    print(\"****************\" * 5)\n",
    "    eos_token = processor.tokenizer.eos_token\n",
    "    target_token = target + eos_token\n",
    "    print(\"eos_token:\", eos_token)\n",
    "    conv = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Can you tell me what this image is about?\"},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conv, add_generation_prompt=add_generation_prompt)\n",
    "    print(prompt)\n",
    "    print()\n",
    "    inputs = processor(text=prompt, images=test_image, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "    print(inputs.keys())\n",
    "    print(inputs[\"input_ids\"].shape)\n",
    "    print((inputs[\"attention_mask\"] == 1).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the (Instruct) Blip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(blip.chat_template)\n",
    "print(instructblip.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])\n",
      "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s>What is unusual about this image? Answer:']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is unusual about this image? Answer:\"\n",
    "inputs = blip(images=test_image, text=prompt, return_tensors=\"pt\", do_rescale=False).to(\"cuda\")\n",
    "print(inputs.keys())\n",
    "print(blip.batch_decode(inputs[\"input_ids\"]))\n",
    "# inputs = instructblip(images=test_image, text=prompt, return_tensors=\"pt\", do_rescale=False)\n",
    "# print(inputs.keys())\n",
    "# print(instructblip.batch_decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_blip = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", device_map=\"auto\", revision=\"51572668da0eb669e01a189dc22abe6088589a24\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n",
      "Both `max_new_tokens` (=50) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "output = model_blip.generate(**inputs, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s>What is unusual about this image? Answer: It's not a picture of a person.\\n\"]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blip.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Error while downloading from https://cdn-lfs.hf-mirror.com/repos/fa/52/fa523532eb768e2126266fb7e4f5eeac3f3069f77eab6fb9ef6ef02b07cc2ed5/46d674bda7114f639c411b6341716c03e0f821407b1c4cd58f28aa1c64b34481?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00004-of-00004.safetensors%3B+filename%3D%22model-00004-of-00004.safetensors%22%3B&Expires=1742981724&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mjk4MTcyNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mYS81Mi9mYTUyMzUzMmViNzY4ZTIxMjYyNjZmYjdlNGY1ZWVhYzNmMzA2OWY3N2VhYjZmYjllZjZlZjAyYjA3Y2MyZWQ1LzQ2ZDY3NGJkYTcxMTRmNjM5YzQxMWI2MzQxNzE2YzAzZTBmODIxNDA3YjFjNGNkNThmMjhhYTFjNjRiMzQ0ODE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=J4-BIhABMpasbci0NU3cAXyqivoMQGiZE60nY1AbgCXmFCvKWOHeA%7EFWWgaQR6qKO-KPaIUGhB3TfFSnrwvSlNeGWOd8z-qK6v1dy6XVPZ-uDnpXgNCRbipME-XgOQ%7E7HdUKl-FZxWcDnLuopTiD-E2wvxYwhzN0dkq05kPgxLsQMnB8o6nrSiaUP1NRdx2tI99UwDeSIisg2GDt1A0G9icu%7Ef9-SQcy8F%7EO8w-WE7iKbAZBfGBxIJu4ok%7EnrKitrOmojRGpErmOiPwUhhTN-Ef1xJGrG6xjdUiwBGTVkBDXDqHfdnQlOxPm0M0YJoMmojr-bVqxigaU5mWHYGFwTQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.hf-mirror.com/repos/fa/52/fa523532eb768e2126266fb7e4f5eeac3f3069f77eab6fb9ef6ef02b07cc2ed5/9e7afd5d1bc89391d3a6735ca68e4539a7a65d6d1ae0f5151b883c57b0ee649c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00004.safetensors%3B+filename%3D%22model-00002-of-00004.safetensors%22%3B&Expires=1742981724&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mjk4MTcyNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mYS81Mi9mYTUyMzUzMmViNzY4ZTIxMjYyNjZmYjdlNGY1ZWVhYzNmMzA2OWY3N2VhYjZmYjllZjZlZjAyYjA3Y2MyZWQ1LzllN2FmZDVkMWJjODkzOTFkM2E2NzM1Y2E2OGU0NTM5YTdhNjVkNmQxYWUwZjUxNTFiODgzYzU3YjBlZTY0OWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=u2YJFZkW6fTj48tvL68AzSTEzv2T1EN%7E0Ou0Y1cCJbzfwEh7jiNW2jSAvdIFw011DTRL1oYXJC8SqzmcjijjaretQp6sajWZRzAO2aDJsJznixMEghWpkAkO0XlWT9Oux91m1BfBsE9d11fJVL7ss7rgjlqeKVwSwe151dsgqmQOvXMaPf-lSx0iDOMZ11EcoGvNZQbsEyh6Th8W7uwTxP2nOO2xF5WKpVYbAKCvrSzqJDv%7EUCjzpQ1iQ%7EsWdDfGVNYrGYp7mq3UeNg8JH%7E-aEcYa4rfe4k6vmE597xq-2De1V8u7adOTPmJtFJuOdZYXIiyDCKXe8aF%7EiBWPORRTw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Fetching 4 files: 100%|██████████| 4/4 [1:10:55<00:00, 1063.97s/it]\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [12:59<00:00, 194.87s/it]\n"
     ]
    }
   ],
   "source": [
    "model_instructblip = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/instructblip-vicuna-7b\", device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'qformer_input_ids', 'qformer_attention_mask', 'pixel_values'])\n",
      "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer:']\n",
      "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer: yes</s><s>']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Question: Describe the image. Answer:\"\n",
    "\n",
    "inputs = instructblip(images=test_image, text=prompt, return_tensors=\"pt\", do_rescale=False).to(\"cuda\")\n",
    "print(inputs.keys())\n",
    "print(instructblip.batch_decode(inputs[\"input_ids\"]))\n",
    "\n",
    "output = model_instructblip.generate(**inputs, max_new_tokens=50)\n",
    "print(instructblip.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructblip.tokenizer.encode(instructblip.tokenizer.eos_token, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer: ERROR!</s>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructblip.tokenizer.batch_decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128000, 128006,    882, 128007,    271,   6854,    499,   3371,\n",
       "            757,   1148,    420,     30, 128256, 128009, 128006,  78191, 128007,\n",
       "            271,   6531, 128009]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14431, 29991,     2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "target_id = instructblip.tokenizer.encode(\"ERROR!\", add_special_tokens=False)\n",
    "eos_id = instructblip.tokenizer.encode(instructblip.tokenizer.eos_token, add_special_tokens=False)\n",
    "target_id = target_id + eos_id\n",
    "target_id = torch.tensor(target_id).unsqueeze(0).to(\"cuda\")\n",
    "print(target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = instructblip.tokenizer(\"Question: Describe the image.\", return_tensors=\"pt\", add_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  894, 29901, 20355,   915,   278,  1967, 29889, 14431, 29991]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = instructblip.tokenizer.encode(\"ERROR!\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtarget_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "target_ids + [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id[\"input_ids\"] = torch.cat([input_id[\"input_ids\"], torch.tensor([target_ids])], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer: ERROR!</s>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructblip.tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"attention_mask\"] = torch.ones_like(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer: ERROR!</s><s>']\n"
     ]
    }
   ],
   "source": [
    "output = model_instructblip.generate(**inputs, max_new_tokens=50)\n",
    "print(instructblip.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
