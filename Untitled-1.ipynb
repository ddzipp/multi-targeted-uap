{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 299, 299])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "from config import Config\n",
    "from dataset import collate_fn, load_dataset\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "\n",
    "def get_dataloader(\n",
    "    name: str,\n",
    "    sample_id: torch.Tensor,\n",
    "    targets: dict,\n",
    "    split=\"val\",\n",
    "    transform=None,\n",
    "    shuffle=True,\n",
    "    batch_size=5,\n",
    "):\n",
    "    # Set multi-target labels\n",
    "    dataset = load_dataset(name, split=split, targets=targets, transform=transform)\n",
    "    dataset = Subset(dataset, sample_id.flatten().tolist())\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        collate_fn=collate_fn,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "cfg = Config()\n",
    "dataloader = get_dataloader(\n",
    "    cfg.dataset_name,\n",
    "    cfg.sample_id,\n",
    "    cfg.targets,\n",
    "    split=cfg.split,\n",
    "    batch_size=cfg.batch_size,\n",
    ")\n",
    "\n",
    "# Example use\n",
    "test_image = dataloader.dataset[0][\"image\"].unsqueeze(0)\n",
    "test_image.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/research/sources/multi-targeted-uap/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"\n",
    "from transformers import (\n",
    "    AutoProcessor,\n",
    "    Blip2ForConditionalGeneration,\n",
    "    Blip2Processor,\n",
    "    InstructBlipForConditionalGeneration,\n",
    "    InstructBlipProcessor,\n",
    "    LlavaNextProcessor,\n",
    ")\n",
    "\n",
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "llava = AutoProcessor.from_pretrained(model_id)\n",
    "model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\n",
    "llavanext = LlavaNextProcessor.from_pretrained(model_id)\n",
    "model_id = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
    "qwen = AutoProcessor.from_pretrained(model_id)\n",
    "model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "qwen25 = AutoProcessor.from_pretrained(model_id)\n",
    "model_id = \"meta-llama/Llama-3.2-11B-Vision-Instruct\"\n",
    "llama = AutoProcessor.from_pretrained(model_id)\n",
    "model_id = \"Salesforce/blip2-opt-2.7b\"\n",
    "blip = Blip2Processor.from_pretrained(model_id)\n",
    "model_id = \"Salesforce/instructblip-vicuna-7b\"\n",
    "instructblip = InstructBlipProcessor.from_pretrained(model_id)\n",
    "model_id = \"openvla/openvla-7b\"\n",
    "openvla = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n",
    "\n",
    "processors = [llava, llavanext, qwen, qwen25, llama, blip, instructblip, openvla]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[INST] <image>\\nDescribe the iamge [/INST]',\n",
       " '[INST] <image>\\nDescribe the iamge [/INST]',\n",
       " '[INST] <image>\\nDescribe the iamge [/INST]']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = [\"Describe the iamge\"] * 3\n",
    "targets = [\"ERROR!\", \"WARNING!\", \"MY TARGET!\"]\n",
    "prompts = []\n",
    "for q in questions:\n",
    "    conv = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": q},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    conversation = llava.apply_chat_template(conv, add_generation_prompt=False)\n",
    "    prompts.append(conversation)\n",
    "\n",
    "prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   733, 16289,  ..., 28748, 16289, 28793],\n",
       "        [    1,   733, 16289,  ..., 28748, 16289, 28793],\n",
       "        [    1,   733, 16289,  ..., 28748, 16289, 28793]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]]), 'pixel_values': tensor([[[[[ 9.6684e-01,  9.9604e-01,  1.0106e+00,  ..., -3.1782e-01,\n",
       "            -2.5943e-01, -3.9081e-01],\n",
       "           [ 9.5224e-01,  9.8144e-01,  1.0106e+00,  ..., -3.1782e-01,\n",
       "            -2.5943e-01, -3.4702e-01],\n",
       "           [ 9.8144e-01,  1.0106e+00,  1.0252e+00,  ..., -3.3242e-01,\n",
       "            -2.7403e-01, -2.7403e-01],\n",
       "           ...,\n",
       "           [-3.6162e-01, -5.3680e-01, -5.8059e-01,  ..., -1.5587e+00,\n",
       "            -1.5441e+00, -1.5295e+00],\n",
       "           [-5.9519e-01, -5.9519e-01, -5.9519e-01,  ..., -1.5441e+00,\n",
       "            -1.5441e+00, -1.5295e+00],\n",
       "           [-6.5359e-01, -6.0979e-01, -6.0979e-01,  ..., -1.5149e+00,\n",
       "            -1.5295e+00, -1.5003e+00]],\n",
       "\n",
       "          [[ 1.2645e+00,  1.2945e+00,  1.3095e+00,  ..., -1.0124e-01,\n",
       "            -2.6204e-02, -1.6127e-01],\n",
       "           [ 1.2495e+00,  1.2795e+00,  1.3095e+00,  ..., -1.0124e-01,\n",
       "            -4.1212e-02, -1.0124e-01],\n",
       "           [ 1.2795e+00,  1.3095e+00,  1.3245e+00,  ..., -1.0124e-01,\n",
       "            -4.1212e-02, -4.1212e-02],\n",
       "           ...,\n",
       "           [-2.2130e-01, -4.0140e-01, -4.7644e-01,  ..., -1.5270e+00,\n",
       "            -1.5120e+00, -1.4970e+00],\n",
       "           [-4.7644e-01, -4.7644e-01, -4.9144e-01,  ..., -1.5120e+00,\n",
       "            -1.5120e+00, -1.4970e+00],\n",
       "           [-5.6648e-01, -5.2146e-01, -5.0645e-01,  ..., -1.4820e+00,\n",
       "            -1.4970e+00, -1.4669e+00]],\n",
       "\n",
       "          [[ 1.7477e+00,  1.7762e+00,  1.7762e+00,  ...,  2.9729e-01,\n",
       "             3.1151e-01,  1.5509e-01],\n",
       "           [ 1.7335e+00,  1.7620e+00,  1.7762e+00,  ...,  2.8307e-01,\n",
       "             2.9729e-01,  2.1197e-01],\n",
       "           [ 1.7620e+00,  1.7904e+00,  1.7904e+00,  ...,  2.8307e-01,\n",
       "             3.1151e-01,  2.6885e-01],\n",
       "           ...,\n",
       "           [ 1.2887e-02, -8.6653e-02, -8.6653e-02,  ..., -1.1674e+00,\n",
       "            -1.1816e+00, -1.1674e+00],\n",
       "           [-1.2931e-01, -1.0087e-01, -1.0087e-01,  ..., -1.1532e+00,\n",
       "            -1.1816e+00, -1.1674e+00],\n",
       "           [-1.0087e-01, -1.1509e-01, -1.2931e-01,  ..., -1.1389e+00,\n",
       "            -1.1674e+00, -1.1389e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -9.7475e-01,\n",
       "            -2.0103e-01,  3.9750e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -9.8935e-01,\n",
       "            -1.8644e-01,  3.9750e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -9.7475e-01,\n",
       "            -1.8644e-01,  3.9750e-01],\n",
       "           ...,\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -3.9081e-01,\n",
       "            -3.4702e-01, -3.3242e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -3.9081e-01,\n",
       "            -3.4702e-01, -3.6162e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -3.6162e-01,\n",
       "            -3.3242e-01, -3.7622e-01]],\n",
       "\n",
       "          [[-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -8.2162e-01,\n",
       "            -7.1227e-02,  4.8406e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -8.2162e-01,\n",
       "            -7.1227e-02,  4.8406e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -8.2162e-01,\n",
       "            -7.1227e-02,  4.8406e-01],\n",
       "           ...,\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -3.2636e-01,\n",
       "            -2.8134e-01, -2.6633e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -3.2636e-01,\n",
       "            -2.8134e-01, -2.9634e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -2.9634e-01,\n",
       "            -2.6633e-01, -3.1135e-01]],\n",
       "\n",
       "          [[-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -3.5683e-01,\n",
       "             2.8307e-01,  7.8077e-01],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -3.7105e-01,\n",
       "             2.8307e-01,  7.8077e-01],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -3.7105e-01,\n",
       "             2.8307e-01,  7.8077e-01],\n",
       "           ...,\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -5.8213e-02,\n",
       "            -1.5553e-02,  1.2887e-02],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -5.8213e-02,\n",
       "            -1.5553e-02, -2.9773e-02],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -2.9773e-02,\n",
       "            -1.3329e-03, -5.8213e-02]]],\n",
       "\n",
       "\n",
       "         [[[ 4.4130e-01, -4.0451e-02, -7.5577e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [ 4.5590e-01,  1.2013e-01, -3.1782e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [ 4.5590e-01,  1.4933e-01, -2.1563e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           ...,\n",
       "           [-3.4702e-01, -3.6162e-01, -3.4702e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-4.0541e-01, -3.7622e-01, -2.8862e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-4.3461e-01, -3.3242e-01, -2.0103e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00]],\n",
       "\n",
       "          [[ 5.5910e-01,  1.3888e-01, -5.2146e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [ 5.7411e-01,  3.1897e-01, -7.1227e-02,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [ 5.7411e-01,  3.4899e-01,  1.8820e-02,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           ...,\n",
       "           [-2.8134e-01, -3.1135e-01, -2.8134e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [-3.2636e-01, -2.9634e-01, -2.2130e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [-3.5637e-01, -2.5132e-01, -1.3126e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00]],\n",
       "\n",
       "          [[ 8.6609e-01,  5.3903e-01, -2.9773e-02,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [ 8.8031e-01,  6.9545e-01,  3.8261e-01,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [ 8.8031e-01,  7.2389e-01,  4.5371e-01,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           ...,\n",
       "           [ 1.2887e-02,  1.2887e-02,  5.5547e-02,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [-5.8213e-02, -2.9773e-02,  8.3987e-02,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [-1.1509e-01, -2.9773e-02,  1.1243e-01,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 9.6684e-01,  9.9604e-01,  1.0106e+00,  ..., -3.1782e-01,\n",
       "            -2.5943e-01, -3.9081e-01],\n",
       "           [ 9.5224e-01,  9.8144e-01,  1.0106e+00,  ..., -3.1782e-01,\n",
       "            -2.5943e-01, -3.4702e-01],\n",
       "           [ 9.8144e-01,  1.0106e+00,  1.0252e+00,  ..., -3.3242e-01,\n",
       "            -2.7403e-01, -2.7403e-01],\n",
       "           ...,\n",
       "           [-3.6162e-01, -5.3680e-01, -5.8059e-01,  ..., -1.5587e+00,\n",
       "            -1.5441e+00, -1.5295e+00],\n",
       "           [-5.9519e-01, -5.9519e-01, -5.9519e-01,  ..., -1.5441e+00,\n",
       "            -1.5441e+00, -1.5295e+00],\n",
       "           [-6.5359e-01, -6.0979e-01, -6.0979e-01,  ..., -1.5149e+00,\n",
       "            -1.5295e+00, -1.5003e+00]],\n",
       "\n",
       "          [[ 1.2645e+00,  1.2945e+00,  1.3095e+00,  ..., -1.0124e-01,\n",
       "            -2.6204e-02, -1.6127e-01],\n",
       "           [ 1.2495e+00,  1.2795e+00,  1.3095e+00,  ..., -1.0124e-01,\n",
       "            -4.1212e-02, -1.0124e-01],\n",
       "           [ 1.2795e+00,  1.3095e+00,  1.3245e+00,  ..., -1.0124e-01,\n",
       "            -4.1212e-02, -4.1212e-02],\n",
       "           ...,\n",
       "           [-2.2130e-01, -4.0140e-01, -4.7644e-01,  ..., -1.5270e+00,\n",
       "            -1.5120e+00, -1.4970e+00],\n",
       "           [-4.7644e-01, -4.7644e-01, -4.9144e-01,  ..., -1.5120e+00,\n",
       "            -1.5120e+00, -1.4970e+00],\n",
       "           [-5.6648e-01, -5.2146e-01, -5.0645e-01,  ..., -1.4820e+00,\n",
       "            -1.4970e+00, -1.4669e+00]],\n",
       "\n",
       "          [[ 1.7477e+00,  1.7762e+00,  1.7762e+00,  ...,  2.9729e-01,\n",
       "             3.1151e-01,  1.5509e-01],\n",
       "           [ 1.7335e+00,  1.7620e+00,  1.7762e+00,  ...,  2.8307e-01,\n",
       "             2.9729e-01,  2.1197e-01],\n",
       "           [ 1.7620e+00,  1.7904e+00,  1.7904e+00,  ...,  2.8307e-01,\n",
       "             3.1151e-01,  2.6885e-01],\n",
       "           ...,\n",
       "           [ 1.2887e-02, -8.6653e-02, -8.6653e-02,  ..., -1.1674e+00,\n",
       "            -1.1816e+00, -1.1674e+00],\n",
       "           [-1.2931e-01, -1.0087e-01, -1.0087e-01,  ..., -1.1532e+00,\n",
       "            -1.1816e+00, -1.1674e+00],\n",
       "           [-1.0087e-01, -1.1509e-01, -1.2931e-01,  ..., -1.1389e+00,\n",
       "            -1.1674e+00, -1.1389e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -9.7475e-01,\n",
       "            -2.0103e-01,  3.9750e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -9.8935e-01,\n",
       "            -1.8644e-01,  3.9750e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -9.7475e-01,\n",
       "            -1.8644e-01,  3.9750e-01],\n",
       "           ...,\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -3.9081e-01,\n",
       "            -3.4702e-01, -3.3242e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -3.9081e-01,\n",
       "            -3.4702e-01, -3.6162e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -3.6162e-01,\n",
       "            -3.3242e-01, -3.7622e-01]],\n",
       "\n",
       "          [[-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -8.2162e-01,\n",
       "            -7.1227e-02,  4.8406e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -8.2162e-01,\n",
       "            -7.1227e-02,  4.8406e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -8.2162e-01,\n",
       "            -7.1227e-02,  4.8406e-01],\n",
       "           ...,\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -3.2636e-01,\n",
       "            -2.8134e-01, -2.6633e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -3.2636e-01,\n",
       "            -2.8134e-01, -2.9634e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -2.9634e-01,\n",
       "            -2.6633e-01, -3.1135e-01]],\n",
       "\n",
       "          [[-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -3.5683e-01,\n",
       "             2.8307e-01,  7.8077e-01],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -3.7105e-01,\n",
       "             2.8307e-01,  7.8077e-01],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -3.7105e-01,\n",
       "             2.8307e-01,  7.8077e-01],\n",
       "           ...,\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -5.8213e-02,\n",
       "            -1.5553e-02,  1.2887e-02],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -5.8213e-02,\n",
       "            -1.5553e-02, -2.9773e-02],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -2.9773e-02,\n",
       "            -1.3329e-03, -5.8213e-02]]],\n",
       "\n",
       "\n",
       "         [[[ 4.4130e-01, -4.0451e-02, -7.5577e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [ 4.5590e-01,  1.2013e-01, -3.1782e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [ 4.5590e-01,  1.4933e-01, -2.1563e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           ...,\n",
       "           [-3.4702e-01, -3.6162e-01, -3.4702e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-4.0541e-01, -3.7622e-01, -2.8862e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-4.3461e-01, -3.3242e-01, -2.0103e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00]],\n",
       "\n",
       "          [[ 5.5910e-01,  1.3888e-01, -5.2146e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [ 5.7411e-01,  3.1897e-01, -7.1227e-02,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [ 5.7411e-01,  3.4899e-01,  1.8820e-02,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           ...,\n",
       "           [-2.8134e-01, -3.1135e-01, -2.8134e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [-3.2636e-01, -2.9634e-01, -2.2130e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [-3.5637e-01, -2.5132e-01, -1.3126e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00]],\n",
       "\n",
       "          [[ 8.6609e-01,  5.3903e-01, -2.9773e-02,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [ 8.8031e-01,  6.9545e-01,  3.8261e-01,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [ 8.8031e-01,  7.2389e-01,  4.5371e-01,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           ...,\n",
       "           [ 1.2887e-02,  1.2887e-02,  5.5547e-02,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [-5.8213e-02, -2.9773e-02,  8.3987e-02,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [-1.1509e-01, -2.9773e-02,  1.1243e-01,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00]]]],\n",
       "\n",
       "\n",
       "\n",
       "        [[[[ 9.6684e-01,  9.9604e-01,  1.0106e+00,  ..., -3.1782e-01,\n",
       "            -2.5943e-01, -3.9081e-01],\n",
       "           [ 9.5224e-01,  9.8144e-01,  1.0106e+00,  ..., -3.1782e-01,\n",
       "            -2.5943e-01, -3.4702e-01],\n",
       "           [ 9.8144e-01,  1.0106e+00,  1.0252e+00,  ..., -3.3242e-01,\n",
       "            -2.7403e-01, -2.7403e-01],\n",
       "           ...,\n",
       "           [-3.6162e-01, -5.3680e-01, -5.8059e-01,  ..., -1.5587e+00,\n",
       "            -1.5441e+00, -1.5295e+00],\n",
       "           [-5.9519e-01, -5.9519e-01, -5.9519e-01,  ..., -1.5441e+00,\n",
       "            -1.5441e+00, -1.5295e+00],\n",
       "           [-6.5359e-01, -6.0979e-01, -6.0979e-01,  ..., -1.5149e+00,\n",
       "            -1.5295e+00, -1.5003e+00]],\n",
       "\n",
       "          [[ 1.2645e+00,  1.2945e+00,  1.3095e+00,  ..., -1.0124e-01,\n",
       "            -2.6204e-02, -1.6127e-01],\n",
       "           [ 1.2495e+00,  1.2795e+00,  1.3095e+00,  ..., -1.0124e-01,\n",
       "            -4.1212e-02, -1.0124e-01],\n",
       "           [ 1.2795e+00,  1.3095e+00,  1.3245e+00,  ..., -1.0124e-01,\n",
       "            -4.1212e-02, -4.1212e-02],\n",
       "           ...,\n",
       "           [-2.2130e-01, -4.0140e-01, -4.7644e-01,  ..., -1.5270e+00,\n",
       "            -1.5120e+00, -1.4970e+00],\n",
       "           [-4.7644e-01, -4.7644e-01, -4.9144e-01,  ..., -1.5120e+00,\n",
       "            -1.5120e+00, -1.4970e+00],\n",
       "           [-5.6648e-01, -5.2146e-01, -5.0645e-01,  ..., -1.4820e+00,\n",
       "            -1.4970e+00, -1.4669e+00]],\n",
       "\n",
       "          [[ 1.7477e+00,  1.7762e+00,  1.7762e+00,  ...,  2.9729e-01,\n",
       "             3.1151e-01,  1.5509e-01],\n",
       "           [ 1.7335e+00,  1.7620e+00,  1.7762e+00,  ...,  2.8307e-01,\n",
       "             2.9729e-01,  2.1197e-01],\n",
       "           [ 1.7620e+00,  1.7904e+00,  1.7904e+00,  ...,  2.8307e-01,\n",
       "             3.1151e-01,  2.6885e-01],\n",
       "           ...,\n",
       "           [ 1.2887e-02, -8.6653e-02, -8.6653e-02,  ..., -1.1674e+00,\n",
       "            -1.1816e+00, -1.1674e+00],\n",
       "           [-1.2931e-01, -1.0087e-01, -1.0087e-01,  ..., -1.1532e+00,\n",
       "            -1.1816e+00, -1.1674e+00],\n",
       "           [-1.0087e-01, -1.1509e-01, -1.2931e-01,  ..., -1.1389e+00,\n",
       "            -1.1674e+00, -1.1389e+00]]],\n",
       "\n",
       "\n",
       "         [[[-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -9.7475e-01,\n",
       "            -2.0103e-01,  3.9750e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -9.8935e-01,\n",
       "            -1.8644e-01,  3.9750e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -9.7475e-01,\n",
       "            -1.8644e-01,  3.9750e-01],\n",
       "           ...,\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -3.9081e-01,\n",
       "            -3.4702e-01, -3.3242e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -3.9081e-01,\n",
       "            -3.4702e-01, -3.6162e-01],\n",
       "           [-1.7923e+00, -1.7923e+00, -1.7923e+00,  ..., -3.6162e-01,\n",
       "            -3.3242e-01, -3.7622e-01]],\n",
       "\n",
       "          [[-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -8.2162e-01,\n",
       "            -7.1227e-02,  4.8406e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -8.2162e-01,\n",
       "            -7.1227e-02,  4.8406e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -8.2162e-01,\n",
       "            -7.1227e-02,  4.8406e-01],\n",
       "           ...,\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -3.2636e-01,\n",
       "            -2.8134e-01, -2.6633e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -3.2636e-01,\n",
       "            -2.8134e-01, -2.9634e-01],\n",
       "           [-1.7521e+00, -1.7521e+00, -1.7521e+00,  ..., -2.9634e-01,\n",
       "            -2.6633e-01, -3.1135e-01]],\n",
       "\n",
       "          [[-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -3.5683e-01,\n",
       "             2.8307e-01,  7.8077e-01],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -3.7105e-01,\n",
       "             2.8307e-01,  7.8077e-01],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -3.7105e-01,\n",
       "             2.8307e-01,  7.8077e-01],\n",
       "           ...,\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -5.8213e-02,\n",
       "            -1.5553e-02,  1.2887e-02],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -5.8213e-02,\n",
       "            -1.5553e-02, -2.9773e-02],\n",
       "           [-1.4802e+00, -1.4802e+00, -1.4802e+00,  ..., -2.9773e-02,\n",
       "            -1.3329e-03, -5.8213e-02]]],\n",
       "\n",
       "\n",
       "         [[[ 4.4130e-01, -4.0451e-02, -7.5577e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [ 4.5590e-01,  1.2013e-01, -3.1782e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [ 4.5590e-01,  1.4933e-01, -2.1563e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           ...,\n",
       "           [-3.4702e-01, -3.6162e-01, -3.4702e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-4.0541e-01, -3.7622e-01, -2.8862e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00],\n",
       "           [-4.3461e-01, -3.3242e-01, -2.0103e-01,  ..., -1.7923e+00,\n",
       "            -1.7923e+00, -1.7923e+00]],\n",
       "\n",
       "          [[ 5.5910e-01,  1.3888e-01, -5.2146e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [ 5.7411e-01,  3.1897e-01, -7.1227e-02,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [ 5.7411e-01,  3.4899e-01,  1.8820e-02,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           ...,\n",
       "           [-2.8134e-01, -3.1135e-01, -2.8134e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [-3.2636e-01, -2.9634e-01, -2.2130e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00],\n",
       "           [-3.5637e-01, -2.5132e-01, -1.3126e-01,  ..., -1.7521e+00,\n",
       "            -1.7521e+00, -1.7521e+00]],\n",
       "\n",
       "          [[ 8.6609e-01,  5.3903e-01, -2.9773e-02,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [ 8.8031e-01,  6.9545e-01,  3.8261e-01,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [ 8.8031e-01,  7.2389e-01,  4.5371e-01,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           ...,\n",
       "           [ 1.2887e-02,  1.2887e-02,  5.5547e-02,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [-5.8213e-02, -2.9773e-02,  8.3987e-02,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00],\n",
       "           [-1.1509e-01, -2.9773e-02,  1.1243e-01,  ..., -1.4802e+00,\n",
       "            -1.4802e+00, -1.4802e+00]]]]]), 'image_sizes': tensor([[299, 299],\n",
       "        [299, 299],\n",
       "        [299, 299]])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = llava(\n",
    "    images=[test_image] * 3,\n",
    "    text=prompts,\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    "    do_rescale=False,  # the image is already rescaled to [0, 1]\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = llava(text=targets, padding=True, padding_side=\"right\", return_tensors=\"pt\")[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2],\n",
       "        [2],\n",
       "        [2]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eos_token_ids = torch.tensor(llava.tokenizer.eos_token_ids).unsqueeze(0)\n",
    "eos_token_ids = eos_token_ids.expand(inputs[\"input_ids\"].shape[0], -1)\n",
    "eos_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'add_eos_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[43], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mllava\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_eos_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m t\n",
      "File \u001b[0;32m~/research/sources/multi-targeted-uap/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2887\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2885\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2886\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2887\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2888\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2889\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/research/sources/multi-targeted-uap/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2975\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   2970\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2971\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2972\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2973\u001b[0m         )\n\u001b[1;32m   2974\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[0;32m-> 2975\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2987\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2988\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2989\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2990\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2991\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2992\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2993\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2994\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2995\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[1;32m   2998\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m   2999\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3017\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3018\u001b[0m     )\n",
      "File \u001b[0;32m~/research/sources/multi-targeted-uap/.venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3177\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   3167\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3168\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3169\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3170\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3174\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3175\u001b[0m )\n\u001b[0;32m-> 3177\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3179\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3181\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3182\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3187\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3191\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3193\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3195\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3197\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: PreTrainedTokenizerFast._batch_encode_plus() got an unexpected keyword argument 'add_eos_tokens'"
     ]
    }
   ],
   "source": [
    "t = llava.tokenizer(text=targets, padding=True, padding_side=\"right\", return_tensors=\"pt\", add_eos_tokens=True)[\n",
    "    \"input_ids\"\n",
    "]\n",
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava.tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ң'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llava.tokenizer.convert_ids_to_tokens(31000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1192])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_inputs(processor, add_generation_prompt=False):\n",
    "    print(\"****************\" * 5)\n",
    "    eos_token = processor.tokenizer.eos_token\n",
    "    print(\"eos_token:\", eos_token)\n",
    "    conv = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Can you tell me what this image is about?\"},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [{\"type\": \"text\", \"text\": \"My Answer.\"}],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conv, add_generation_prompt=add_generation_prompt)\n",
    "    print(prompt)\n",
    "    print()\n",
    "    inputs = processor(text=prompt, images=test_image, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "    print(inputs.keys())\n",
    "    print(inputs[\"input_ids\"].shape)\n",
    "    print((inputs[\"attention_mask\"] == 1).all())\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: </s>\n",
      "[INST] <image>\n",
      "Can you tell me what this image is about? [/INST] My Answer.</s> \n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_sizes'])\n",
      "torch.Size([1, 1202])\n",
      "tensor(True)\n",
      "tensor([[28804,   733, 28748, 16289, 28793,  1984, 26307, 28723,     2,   259]])\n",
      "********************************************************************************\n",
      "eos_token: </s>\n",
      "[INST] <image>\n",
      "Can you tell me what this image is about? [/INST] My Answer.</s> \n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_sizes'])\n",
      "torch.Size([1, 1202])\n",
      "tensor(True)\n",
      "tensor([[28804,   733, 28748, 16289, 28793,  1984, 26307, 28723,     2,   259]])\n"
     ]
    }
   ],
   "source": [
    "inputs = print_inputs(llava)\n",
    "print(inputs[\"input_ids\"][:, -10:])\n",
    "inputs = print_inputs(llava, True)\n",
    "print(inputs[\"input_ids\"][:, -10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: </s>\n",
      "[INST] <image>\n",
      "Can you tell me what this image is about? [/INST] My Answer.</s> \n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_sizes'])\n",
      "torch.Size([1, 1202])\n",
      "tensor(True)\n",
      "********************************************************************************\n",
      "eos_token: </s>\n",
      "[INST] <image>\n",
      "Can you tell me what this image is about? [/INST] My Answer.</s> \n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_sizes'])\n",
      "torch.Size([1, 1202])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "inputs = print_inputs(llavanext)\n",
    "\n",
    "inputs = print_inputs(llavanext, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: <|im_end|>\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me what this image is about?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "My Answer.<|im_end|>\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
      "torch.Size([1, 157])\n",
      "tensor(True)\n",
      "torch.Size([1, 3])\n",
      "********************************************************************************\n",
      "eos_token: <|im_end|>\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me what this image is about?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "My Answer.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
      "torch.Size([1, 160])\n",
      "tensor(True)\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "inputs = print_inputs(qwen)\n",
    "print(inputs[\"image_grid_thw\"].shape)\n",
    "inputs = print_inputs(qwen, True)\n",
    "print(inputs[\"image_grid_thw\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: <|im_end|>\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me what this image is about?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "My Answer.<|im_end|>\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
      "torch.Size([1, 157])\n",
      "tensor(True)\n",
      "********************************************************************************\n",
      "eos_token: <|im_end|>\n",
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Can you tell me what this image is about?<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "My Answer.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw'])\n",
      "torch.Size([1, 160])\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "print_inputs(qwen25)\n",
    "print_inputs(qwen25, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************************************************************\n",
      "eos_token: <|eot_id|>\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me what this image is about?<|image|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My Answer.<|eot_id|>\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask'])\n",
      "torch.Size([1, 26])\n",
      "tensor(True)\n",
      "torch.Size([1, 26, 1, 4])\n",
      "********************************************************************************\n",
      "eos_token: <|eot_id|>\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me what this image is about?<|image|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "My Answer.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask'])\n",
      "torch.Size([1, 30])\n",
      "tensor(True)\n",
      "torch.Size([1, 30, 1, 4])\n"
     ]
    }
   ],
   "source": [
    "inputs = print_inputs(llama)\n",
    "print(inputs[\"cross_attention_mask\"].shape)\n",
    "inputs = print_inputs(llama, True)\n",
    "print(inputs[\"cross_attention_mask\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(openvla.chat_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze Target Token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me what this?<|image|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'aspect_ratio_ids', 'aspect_ratio_mask', 'cross_attention_mask'])\n",
      "tensor([[   757,   1148,    420,     30, 128256, 128009, 128006,  78191, 128007,\n",
      "            271]])\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "Can you tell me what this?<|image|><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "Target<|eot_id|>\n",
      "tensor([[   420,     30, 128256, 128009, 128006,  78191, 128007,    271,   6531,\n",
      "         128009]])\n",
      "***************\n",
      "Error input_ids\n",
      "Error attention_mask\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n",
      "Error cross_attention_mask\n"
     ]
    }
   ],
   "source": [
    "processor = llama\n",
    "conv = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Can you tell me what this?\"},\n",
    "            {\"type\": \"image\"},\n",
    "        ],\n",
    "    },\n",
    "]\n",
    "\n",
    "prompt = processor.apply_chat_template(conv, add_generation_prompt=True)\n",
    "print(prompt)\n",
    "inputs0 = processor(images=test_image, text=prompt, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "print(inputs0.keys())\n",
    "print(inputs0[\"input_ids\"][:, -10:])\n",
    "\n",
    "\n",
    "conv.append(\n",
    "    {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"Target\"},\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "prompt = processor.apply_chat_template(conv, add_generation_prompt=False)\n",
    "print(prompt)\n",
    "inputs1 = processor(images=test_image, text=prompt, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "print(inputs1[\"input_ids\"][:, -10:])\n",
    "\n",
    "print(\"***************\")\n",
    "for key in inputs1.keys():\n",
    "    try:\n",
    "        print((inputs0[key] == inputs1[key]).all())\n",
    "    except Exception:\n",
    "        print(\"Error\", key)\n",
    "# target = \"Target\" + processor.tokenizer.eos_token\n",
    "# inputs = processor.tokenizer.encode(target, add_special_tokens=False)\n",
    "# print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(inputs0[\"cross_attention_mask\"] == inputs1[\"cross_attention_mask\"][:, :19]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1, 0, 0, 0]],\n",
       "\n",
       "         [[1, 0, 0, 0]]]])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs1[\"cross_attention_mask\"][:, 19:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "openvla.tokenizer.eos_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_inputs(processor, add_generation_prompt=True, target=\"Target\"):\n",
    "    print(\"****************\" * 5)\n",
    "    eos_token = processor.tokenizer.eos_token\n",
    "    target_token = target + eos_token\n",
    "    print(\"eos_token:\", eos_token)\n",
    "    conv = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": \"Can you tell me what this image is about?\"},\n",
    "                {\"type\": \"image\"},\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "    prompt = processor.apply_chat_template(conv, add_generation_prompt=add_generation_prompt)\n",
    "    print(prompt)\n",
    "    print()\n",
    "    inputs = processor(text=prompt, images=test_image, return_tensors=\"pt\", padding=True, do_rescale=False)\n",
    "    print(inputs.keys())\n",
    "    print(inputs[\"input_ids\"].shape)\n",
    "    print((inputs[\"attention_mask\"] == 1).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the (Instruct) Blip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(blip.chat_template)\n",
    "print(instructblip.chat_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'pixel_values'])\n",
      "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s>What is unusual about this image? Answer:']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"What is unusual about this image? Answer:\"\n",
    "inputs = blip(images=test_image, text=prompt, return_tensors=\"pt\", do_rescale=False).to(\"cuda\")\n",
    "print(inputs.keys())\n",
    "print(blip.batch_decode(inputs[\"input_ids\"]))\n",
    "# inputs = instructblip(images=test_image, text=prompt, return_tensors=\"pt\", do_rescale=False)\n",
    "# print(inputs.keys())\n",
    "# print(instructblip.batch_decode(inputs[\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_blip = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/blip2-opt-2.7b\", device_map=\"auto\", revision=\"51572668da0eb669e01a189dc22abe6088589a24\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n",
      "Both `max_new_tokens` (=50) and `max_length`(=51) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    }
   ],
   "source": [
    "output = model_blip.generate(**inputs, max_new_tokens=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s>What is unusual about this image? Answer: It's not a picture of a person.\\n\"]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blip.batch_decode(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Error while downloading from https://cdn-lfs.hf-mirror.com/repos/fa/52/fa523532eb768e2126266fb7e4f5eeac3f3069f77eab6fb9ef6ef02b07cc2ed5/46d674bda7114f639c411b6341716c03e0f821407b1c4cd58f28aa1c64b34481?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00004-of-00004.safetensors%3B+filename%3D%22model-00004-of-00004.safetensors%22%3B&Expires=1742981724&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mjk4MTcyNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mYS81Mi9mYTUyMzUzMmViNzY4ZTIxMjYyNjZmYjdlNGY1ZWVhYzNmMzA2OWY3N2VhYjZmYjllZjZlZjAyYjA3Y2MyZWQ1LzQ2ZDY3NGJkYTcxMTRmNjM5YzQxMWI2MzQxNzE2YzAzZTBmODIxNDA3YjFjNGNkNThmMjhhYTFjNjRiMzQ0ODE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=J4-BIhABMpasbci0NU3cAXyqivoMQGiZE60nY1AbgCXmFCvKWOHeA%7EFWWgaQR6qKO-KPaIUGhB3TfFSnrwvSlNeGWOd8z-qK6v1dy6XVPZ-uDnpXgNCRbipME-XgOQ%7E7HdUKl-FZxWcDnLuopTiD-E2wvxYwhzN0dkq05kPgxLsQMnB8o6nrSiaUP1NRdx2tI99UwDeSIisg2GDt1A0G9icu%7Ef9-SQcy8F%7EO8w-WE7iKbAZBfGBxIJu4ok%7EnrKitrOmojRGpErmOiPwUhhTN-Ef1xJGrG6xjdUiwBGTVkBDXDqHfdnQlOxPm0M0YJoMmojr-bVqxigaU5mWHYGFwTQ__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Error while downloading from https://cdn-lfs.hf-mirror.com/repos/fa/52/fa523532eb768e2126266fb7e4f5eeac3f3069f77eab6fb9ef6ef02b07cc2ed5/9e7afd5d1bc89391d3a6735ca68e4539a7a65d6d1ae0f5151b883c57b0ee649c?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00004.safetensors%3B+filename%3D%22model-00002-of-00004.safetensors%22%3B&Expires=1742981724&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0Mjk4MTcyNH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy9mYS81Mi9mYTUyMzUzMmViNzY4ZTIxMjYyNjZmYjdlNGY1ZWVhYzNmMzA2OWY3N2VhYjZmYjllZjZlZjAyYjA3Y2MyZWQ1LzllN2FmZDVkMWJjODkzOTFkM2E2NzM1Y2E2OGU0NTM5YTdhNjVkNmQxYWUwZjUxNTFiODgzYzU3YjBlZTY0OWM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=u2YJFZkW6fTj48tvL68AzSTEzv2T1EN%7E0Ou0Y1cCJbzfwEh7jiNW2jSAvdIFw011DTRL1oYXJC8SqzmcjijjaretQp6sajWZRzAO2aDJsJznixMEghWpkAkO0XlWT9Oux91m1BfBsE9d11fJVL7ss7rgjlqeKVwSwe151dsgqmQOvXMaPf-lSx0iDOMZ11EcoGvNZQbsEyh6Th8W7uwTxP2nOO2xF5WKpVYbAKCvrSzqJDv%7EUCjzpQ1iQ%7EsWdDfGVNYrGYp7mq3UeNg8JH%7E-aEcYa4rfe4k6vmE597xq-2De1V8u7adOTPmJtFJuOdZYXIiyDCKXe8aF%7EiBWPORRTw__&Key-Pair-Id=K3RPWS32NSSJCE: HTTPSConnectionPool(host='cdn-lfs.hf-mirror.com', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Fetching 4 files: 100%|██████████| 4/4 [1:10:55<00:00, 1063.97s/it]\n",
      "The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [12:59<00:00, 194.87s/it]\n"
     ]
    }
   ],
   "source": [
    "model_instructblip = InstructBlipForConditionalGeneration.from_pretrained(\n",
    "    \"Salesforce/instructblip-vicuna-7b\", device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'qformer_input_ids', 'qformer_attention_mask', 'pixel_values'])\n",
      "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer:']\n",
      "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer: yes</s><s>']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Question: Describe the image. Answer:\"\n",
    "\n",
    "inputs = instructblip(images=test_image, text=prompt, return_tensors=\"pt\", do_rescale=False).to(\"cuda\")\n",
    "print(inputs.keys())\n",
    "print(instructblip.batch_decode(inputs[\"input_ids\"]))\n",
    "\n",
    "output = model_instructblip.generate(**inputs, max_new_tokens=50)\n",
    "print(instructblip.batch_decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructblip.tokenizer.encode(instructblip.tokenizer.eos_token, add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer: ERROR!</s>']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructblip.tokenizer.batch_decode(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000, 128000, 128006,    882, 128007,    271,   6854,    499,   3371,\n",
       "            757,   1148,    420,     30, 128256, 128009, 128006,  78191, 128007,\n",
       "            271,   6531, 128009]])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14431, 29991,     2]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "target_id = instructblip.tokenizer.encode(\"ERROR!\", add_special_tokens=False)\n",
    "eos_id = instructblip.tokenizer.encode(instructblip.tokenizer.eos_token, add_special_tokens=False)\n",
    "target_id = target_id + eos_id\n",
    "target_id = torch.tensor(target_id).unsqueeze(0).to(\"cuda\")\n",
    "print(target_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id = instructblip.tokenizer(\"Question: Describe the image.\", return_tensors=\"pt\", add_special_tokens=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  894, 29901, 20355,   915,   278,  1967, 29889, 14431, 29991]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_id[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = instructblip.tokenizer.encode(\"ERROR!\", add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"int\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[144], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtarget_ids\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"int\") to list"
     ]
    }
   ],
   "source": [
    "target_ids + [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_id[\"input_ids\"] = torch.cat([input_id[\"input_ids\"], torch.tensor([target_ids])], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer: ERROR!</s>'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructblip.tokenizer.decode(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[\"attention_mask\"] = torch.ones_like(inputs[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `language_model` is not in the `hf_device_map` dictionary and you are running your script in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`. Please pass a `device_map` that contains `language_model` to remove this warning. Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for more details on creating a `device_map` for large models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image><image></s> Question: Describe the image. Answer: ERROR!</s><s>']\n"
     ]
    }
   ],
   "source": [
    "output = model_instructblip.generate(**inputs, max_new_tokens=50)\n",
    "print(instructblip.batch_decode(output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagenet = load_dataset(\"ImageNet\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import json\n",
    "\n",
    "c = Counter(imagenet.dataset.targets)\n",
    "\n",
    "start_idx = torch.zeros(1000).to(int)\n",
    "for i in range(1, 1000):\n",
    "    start_idx[i] = c[i - 1] + start_idx[i - 1]\n",
    "\n",
    "for i in range(1, 1000):\n",
    "    a = imagenet.dataset.targets[start_idx[i] - 1]\n",
    "    b = imagenet.dataset.targets[start_idx[i]]\n",
    "    assert a != b\n",
    "\n",
    "\n",
    "json.dump(start_idx, open(\"./data/ImageNet/imagenet_train_start_idx.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(849488)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_idx[662]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_idx = {str(x): y.item() for x, y in zip(range(1000), start_idx)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492 366\n",
      "153 524\n"
     ]
    }
   ],
   "source": [
    "import torch, random\n",
    "\n",
    "for x, y in torch.tensor(random.sample(range(1000), 4)).reshape(2, 2):\n",
    "    print(x.item(), y.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import asdict, dataclass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Config Class\n",
    "    Support dataclass, config file, and command line arguments\n",
    "    Returns:\n",
    "        _type_: dataclass\n",
    "    \"\"\"\n",
    "\n",
    "    # dataset info\n",
    "    # dataset_name: str = \"VQA\"\n",
    "    dataset_name: str = \"ImageNet\"\n",
    "    split: str = \"train\"\n",
    "    # targets: torch.Tensor = torch.randperm(1000)[:2]\n",
    "    targets: dict | None = None\n",
    "    sample_id: torch.Tensor | None = None\n",
    "    batch_size: int = 10\n",
    "    # model info\n",
    "    model_name: str = \"densenet121\"  # renset50, llava\n",
    "\n",
    "    # attack info\n",
    "    attack_name: str = \"base\"  # base, split, union_split\n",
    "    lr: float = 0.01 * batch_size\n",
    "    epoch: int = 500\n",
    "    attack_mode: str = \"frame\"\n",
    "    bound: tuple = (0, 1)\n",
    "    # bound: tuple = (-16 / 255, 16 / 255)\n",
    "    frame_width: int = 6\n",
    "    patch_size: tuple[int, int] = (40, 40)\n",
    "    patch_location: tuple[int, int] = (0, 0)\n",
    "    on_normalized: bool = True\n",
    "\n",
    "    def asdict(self):\n",
    "        return asdict(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.targets = {\"1\": \"0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_name': 'ImageNet',\n",
       " 'split': 'train',\n",
       " 'targets': {'1': '0'},\n",
       " 'sample_id': None,\n",
       " 'batch_size': 10,\n",
       " 'model_name': 'densenet121',\n",
       " 'attack_name': 'base',\n",
       " 'lr': 0.1,\n",
       " 'epoch': 500,\n",
       " 'attack_mode': 'frame',\n",
       " 'bound': (0, 1),\n",
       " 'frame_width': 6,\n",
       " 'patch_size': (40, 40),\n",
       " 'patch_location': (0, 0),\n",
       " 'on_normalized': True}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.asdict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
