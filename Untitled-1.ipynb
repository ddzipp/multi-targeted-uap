{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "print(\"Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/research/sources/multi-targeted-uap/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.50, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "Expected `transformers==4.40.1` and `tokenizers==0.19.1` but got `transformers==4.50.0` and `tokenizers==0.21.1`; there might be inference-time regressions due to dependency changes. If in doubt, pleaseuse the above versions.\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [03:49<00:00, 76.43s/it] \n"
     ]
    }
   ],
   "source": [
    "# Install minimal dependencies (`torch`, `transformers`, `timm`, `tokenizers`, ...)\n",
    "# > pip install -r https://raw.githubusercontent.com/openvla/openvla/main/requirements-min.txt\n",
    "import os\n",
    "\n",
    "from transformers import AutoModelForVision2Seq, AutoProcessor\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4,5,6\"  # [Optional] Set visible devices\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load Processor & VLA\n",
    "processor = AutoProcessor.from_pretrained(\"openvla/openvla-7b\", trust_remote_code=True)\n",
    "vla = AutoModelForVision2Seq.from_pretrained(\n",
    "    \"openvla/openvla-7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vla = vla.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n",
    "\n",
    "image = PIL.Image.open(\"./image.png\").convert(\"RGB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"IN: What action should the robot take to pick up the coca-cola can?\\nOUT:\"\n",
    "inputs = processor(\n",
    "    prompt,\n",
    "    image,\n",
    ").to(\"cuda\", dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = vla.predict_action(**inputs, unnorm_key=\"bridge_orig\", do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.47530357e-03, -7.47882333e-04,  7.03386392e-03, -9.71779424e-03,\n",
       "        1.89886098e-03, -3.29030749e-03,  9.96078431e-01])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrismaticProcessor:\n",
       "- image_processor: PrismaticImageProcessor {\n",
       "  \"auto_map\": {\n",
       "    \"AutoImageProcessor\": \"openvla/openvla-7b--processing_prismatic.PrismaticImageProcessor\",\n",
       "    \"AutoProcessor\": \"openvla/openvla-7b--processing_prismatic.PrismaticProcessor\"\n",
       "  },\n",
       "  \"image_processor_type\": \"PrismaticImageProcessor\",\n",
       "  \"image_resize_strategy\": \"resize-naive\",\n",
       "  \"input_sizes\": [\n",
       "    [\n",
       "      3,\n",
       "      224,\n",
       "      224\n",
       "    ],\n",
       "    [\n",
       "      3,\n",
       "      224,\n",
       "      224\n",
       "    ]\n",
       "  ],\n",
       "  \"interpolations\": [\n",
       "    \"bicubic\",\n",
       "    \"bicubic\"\n",
       "  ],\n",
       "  \"means\": [\n",
       "    [\n",
       "      0.485,\n",
       "      0.456,\n",
       "      0.406\n",
       "    ],\n",
       "    [\n",
       "      0.5,\n",
       "      0.5,\n",
       "      0.5\n",
       "    ]\n",
       "  ],\n",
       "  \"processor_class\": \"PrismaticProcessor\",\n",
       "  \"stds\": [\n",
       "    [\n",
       "      0.229,\n",
       "      0.224,\n",
       "      0.225\n",
       "    ],\n",
       "    [\n",
       "      0.5,\n",
       "      0.5,\n",
       "      0.5\n",
       "    ]\n",
       "  ],\n",
       "  \"tvf_crop_params\": [\n",
       "    {\n",
       "      \"output_size\": [\n",
       "        224,\n",
       "        224\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"output_size\": [\n",
       "        224,\n",
       "        224\n",
       "      ]\n",
       "    }\n",
       "  ],\n",
       "  \"tvf_do_letterbox\": false,\n",
       "  \"tvf_letterbox_fill\": null,\n",
       "  \"tvf_normalize_params\": [\n",
       "    {\n",
       "      \"inplace\": false,\n",
       "      \"mean\": [\n",
       "        0.484375,\n",
       "        0.455078125,\n",
       "        0.40625\n",
       "      ],\n",
       "      \"std\": [\n",
       "        0.228515625,\n",
       "        0.2236328125,\n",
       "        0.224609375\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"inplace\": false,\n",
       "      \"mean\": [\n",
       "        0.5,\n",
       "        0.5,\n",
       "        0.5\n",
       "      ],\n",
       "      \"std\": [\n",
       "        0.5,\n",
       "        0.5,\n",
       "        0.5\n",
       "      ]\n",
       "    }\n",
       "  ],\n",
       "  \"tvf_resize_params\": [\n",
       "    {\n",
       "      \"antialias\": true,\n",
       "      \"interpolation\": 3,\n",
       "      \"max_size\": null,\n",
       "      \"size\": [\n",
       "        224,\n",
       "        224\n",
       "      ]\n",
       "    },\n",
       "    {\n",
       "      \"antialias\": true,\n",
       "      \"interpolation\": 3,\n",
       "      \"max_size\": null,\n",
       "      \"size\": [\n",
       "        224,\n",
       "        224\n",
       "      ]\n",
       "    }\n",
       "  ],\n",
       "  \"use_fused_vision_backbone\": true\n",
       "}\n",
       "\n",
       "- tokenizer: LlamaTokenizerFast(name_or_path='openvla/openvla-7b', vocab_size=32000, model_max_length=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<PAD>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<PAD>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")\n",
       "\n",
       "{\n",
       "  \"processor_class\": \"PrismaticProcessor\"\n",
       "}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = timm.create_model(\"resnet50\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cfg = timm.data.resolve_data_config(model.pretrained_cfg)\n",
    "transform = timm.data.create_transform(**data_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Compose(\n",
       "    Resize(size=235, interpolation=bicubic, max_size=None, antialias=True)\n",
       "    CenterCrop(size=(224, 224))\n",
       "    ToTensor()\n",
       "    Normalize(mean=tensor([0.4850, 0.4560, 0.4060]), std=tensor([0.2290, 0.2240, 0.2250]))\n",
       ")"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
